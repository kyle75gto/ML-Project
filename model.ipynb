{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing useful libraries\n",
    "import pandas as pd\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#importing SNP dataframe\n",
    "data = yf.download('^GSPC', start = '2018-11-14', end='2024-11-15')\n",
    "data.to_csv('snp500_one_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1510, 6)\n"
     ]
    }
   ],
   "source": [
    "# display dataframe information\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price                        Adj Close        Close         High          Low  \\\n",
      "Ticker                           ^GSPC        ^GSPC        ^GSPC        ^GSPC   \n",
      "Date                                                                            \n",
      "2024-11-08 00:00:00+00:00  5995.540039  5995.540039  6012.450195  5976.759766   \n",
      "2024-11-11 00:00:00+00:00  6001.350098  6001.350098  6017.310059  5986.689941   \n",
      "2024-11-12 00:00:00+00:00  5983.990234  5983.990234  6009.919922  5960.080078   \n",
      "2024-11-13 00:00:00+00:00  5985.379883  5985.379883  6008.189941  5965.910156   \n",
      "2024-11-14 00:00:00+00:00  5949.169922  5949.169922  5993.879883  5942.279785   \n",
      "\n",
      "Price                             Open      Volume  \n",
      "Ticker                           ^GSPC       ^GSPC  \n",
      "Date                                                \n",
      "2024-11-08 00:00:00+00:00  5976.759766  4666740000  \n",
      "2024-11-11 00:00:00+00:00  6008.859863  4333000000  \n",
      "2024-11-12 00:00:00+00:00  6003.600098  4243400000  \n",
      "2024-11-13 00:00:00+00:00  5985.750000  4220180000  \n",
      "2024-11-14 00:00:00+00:00  5989.680176  4184570000  \n"
     ]
    }
   ],
   "source": [
    "# display tail of df\n",
    "print(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of non useful columns\n",
    "data = data.drop(columns = [\"Open\",\"High\",\"Low\",\"Close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([('Adj Close', '^GSPC'),\n",
       "            (   'Volume', '^GSPC')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the indexes\n",
    "data = data.reset_index()\n",
    "data = data[['Date', 'Adj Close', 'Volume']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the date format\n",
    "data['Date'] = pd.to_datetime(data['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(     'Date',      ''),\n",
       "            ('Adj Close', '^GSPC'),\n",
       "            (   'Volume', '^GSPC')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the dataset columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the columns of the dataset\n",
    "data.columns = ['Date', 'Adj Close', 'Volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index of the dataset\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the returns to be able to calculate other indicators (vol...)\n",
    "data['Returns'] = data['Adj Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# import the VIX data from yfinance\n",
    "vix_data = yf.download('^VIX', start='2018-11-14', end='2024-11-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price                      Adj Close      Close       High        Low  \\\n",
      "Ticker                          ^VIX       ^VIX       ^VIX       ^VIX   \n",
      "Date                                                                    \n",
      "2018-11-14 00:00:00+00:00  21.250000  21.250000  22.360001  19.299999   \n",
      "2018-11-15 00:00:00+00:00  19.980000  19.980000  22.969999  19.940001   \n",
      "2018-11-16 00:00:00+00:00  18.139999  18.139999  21.360001  18.100000   \n",
      "2018-11-19 00:00:00+00:00  20.100000  20.100000  20.990000  18.520000   \n",
      "2018-11-20 00:00:00+00:00  22.480000  22.480000  23.809999  20.370001   \n",
      "...                              ...        ...        ...        ...   \n",
      "2024-11-08 00:00:00+00:00  14.940000  14.940000  15.330000  14.660000   \n",
      "2024-11-11 00:00:00+00:00  14.970000  14.970000  15.560000  14.890000   \n",
      "2024-11-12 00:00:00+00:00  14.710000  14.710000  15.370000  14.690000   \n",
      "2024-11-13 00:00:00+00:00  14.020000  14.020000  15.260000  13.770000   \n",
      "2024-11-14 00:00:00+00:00  14.310000  14.310000  14.320000  13.590000   \n",
      "\n",
      "Price                           Open Volume  \n",
      "Ticker                          ^VIX   ^VIX  \n",
      "Date                                         \n",
      "2018-11-14 00:00:00+00:00  20.520000      0  \n",
      "2018-11-15 00:00:00+00:00  20.410000      0  \n",
      "2018-11-16 00:00:00+00:00  20.040001      0  \n",
      "2018-11-19 00:00:00+00:00  18.780001      0  \n",
      "2018-11-20 00:00:00+00:00  20.760000      0  \n",
      "...                              ...    ...  \n",
      "2024-11-08 00:00:00+00:00  15.130000      0  \n",
      "2024-11-11 00:00:00+00:00  15.330000      0  \n",
      "2024-11-12 00:00:00+00:00  15.090000      0  \n",
      "2024-11-13 00:00:00+00:00  15.090000      0  \n",
      "2024-11-14 00:00:00+00:00  14.170000      0  \n",
      "\n",
      "[1510 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# display the vix dataset\n",
    "print(vix_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^VIX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-14 00:00:00+00:00</th>\n",
       "      <td>21.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15 00:00:00+00:00</th>\n",
       "      <td>19.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16 00:00:00+00:00</th>\n",
       "      <td>18.139999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19 00:00:00+00:00</th>\n",
       "      <td>20.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20 00:00:00+00:00</th>\n",
       "      <td>22.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-08 00:00:00+00:00</th>\n",
       "      <td>14.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-11 00:00:00+00:00</th>\n",
       "      <td>14.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-12 00:00:00+00:00</th>\n",
       "      <td>14.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 00:00:00+00:00</th>\n",
       "      <td>14.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 00:00:00+00:00</th>\n",
       "      <td>14.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                      Adj Close\n",
       "Ticker                          ^VIX\n",
       "Date                                \n",
       "2018-11-14 00:00:00+00:00  21.250000\n",
       "2018-11-15 00:00:00+00:00  19.980000\n",
       "2018-11-16 00:00:00+00:00  18.139999\n",
       "2018-11-19 00:00:00+00:00  20.100000\n",
       "2018-11-20 00:00:00+00:00  22.480000\n",
       "...                              ...\n",
       "2024-11-08 00:00:00+00:00  14.940000\n",
       "2024-11-11 00:00:00+00:00  14.970000\n",
       "2024-11-12 00:00:00+00:00  14.710000\n",
       "2024-11-13 00:00:00+00:00  14.020000\n",
       "2024-11-14 00:00:00+00:00  14.310000\n",
       "\n",
       "[1510 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of the useless columns\n",
    "vix_data.drop(columns=[\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have the same format as our main dataset to merge it correctly\n",
    "vix_data = vix_data.reset_index()\n",
    "vix_data = vix_data[['Date', 'Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the format date\n",
    "vix_data['Date'] = pd.to_datetime(vix_data['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put date as index\n",
    "vix_data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want one column\n",
    "vix_data.columns = ['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging main dataset and vix dataset\n",
    "data = pd.merge(data,vix_data,on=\"Date\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns of the main dataset\n",
    "data = data.rename(columns={\n",
    "    'Adj Close_x': 'S&P 500 Adj Close Price',\n",
    "    'Adj Close_y': 'VIX Adj Close Price',\n",
    "    'Volume': 'Volume S&P 500'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# import the data from yf\n",
    "tnx_data = yf.download('^TNX', start='2018-11-14', end='2024-11-15')\n",
    "fed_data = yf.download('^IRX', start='2018-11-14', end='2024-11-15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>^TNX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-11-14 00:00:00+00:00</th>\n",
       "      <td>3.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-15 00:00:00+00:00</th>\n",
       "      <td>3.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-16 00:00:00+00:00</th>\n",
       "      <td>3.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-19 00:00:00+00:00</th>\n",
       "      <td>3.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20 00:00:00+00:00</th>\n",
       "      <td>3.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-08 00:00:00+00:00</th>\n",
       "      <td>4.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-11 00:00:00+00:00</th>\n",
       "      <td>4.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-12 00:00:00+00:00</th>\n",
       "      <td>4.432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13 00:00:00+00:00</th>\n",
       "      <td>4.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14 00:00:00+00:00</th>\n",
       "      <td>4.418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1510 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                     Adj Close\n",
       "Ticker                         ^TNX\n",
       "Date                               \n",
       "2018-11-14 00:00:00+00:00     3.120\n",
       "2018-11-15 00:00:00+00:00     3.118\n",
       "2018-11-16 00:00:00+00:00     3.074\n",
       "2018-11-19 00:00:00+00:00     3.057\n",
       "2018-11-20 00:00:00+00:00     3.048\n",
       "...                             ...\n",
       "2024-11-08 00:00:00+00:00     4.306\n",
       "2024-11-11 00:00:00+00:00     4.308\n",
       "2024-11-12 00:00:00+00:00     4.432\n",
       "2024-11-13 00:00:00+00:00     4.451\n",
       "2024-11-14 00:00:00+00:00     4.418\n",
       "\n",
       "[1510 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of the useless columns\n",
    "fed_data.drop(columns=[\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"])\n",
    "tnx_data.drop(columns=[\"Close\",\"High\",\"Low\",\"Open\",\"Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the weird index format\n",
    "tnx_data = tnx_data.reset_index()\n",
    "fed_data = fed_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only useful columns\n",
    "tnx_data = tnx_data[['Date', 'Close']]\n",
    "fed_data = fed_data[['Date', 'Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the date format\n",
    "tnx_data['Date'] = pd.to_datetime(tnx_data['Date']).dt.date\n",
    "fed_data['Date'] = pd.to_datetime(fed_data['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tnx_data.set_index('Date', inplace=True)\n",
    "fed_data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want one column\n",
    "tnx_data.columns = ['Adj Close']\n",
    "fed_data.columns = ['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-08</th>\n",
       "      <td>4.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-11</th>\n",
       "      <td>4.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-12</th>\n",
       "      <td>4.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13</th>\n",
       "      <td>4.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14</th>\n",
       "      <td>4.383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Adj Close\n",
       "Date                 \n",
       "2024-11-08      4.423\n",
       "2024-11-11      4.415\n",
       "2024-11-12      4.423\n",
       "2024-11-13      4.390\n",
       "2024-11-14      4.383"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fed_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge main dataset and fed_data\n",
    "data=pd.merge(data,fed_data,on=\"Date\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column in the main dataset\n",
    "data = data.rename(columns={\n",
    "    'Adj Close': 'FED Rates',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge main dataset and tnx_data\n",
    "data = pd.merge(data,tnx_data,on=\"Date\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column in the main dataset\n",
    "data = data.rename(columns={\n",
    "    'Adj Close': '10 Y Treasury Rates',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding Volume Weighted Average Price\n",
    "data[\"VWAP\"] = (data['S&P 500 Adj Close Price'] * data['Volume S&P 500']).cumsum() / data['Volume S&P 500'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding spread between 10 Y Treasury Bunds Rates and FED Rates\n",
    "data['Spread_10Y_Fed'] = data['10 Y Treasury Rates'] - data['FED Rates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding SMA (Simple Moving Average) and EMA (Exponential Moving Average)\n",
    "data['SMA_10'] = data['S&P 500 Adj Close Price'].rolling(window=10).mean()\n",
    "data['EMA_10'] = data['S&P 500 Adj Close Price'].ewm(span=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding Bollinger bounds on 10 last days (volatility measure)\n",
    "rolling_mean = data['S&P 500 Adj Close Price'].rolling(window=10).mean()\n",
    "rolling_std = data['S&P 500 Adj Close Price'].rolling(window=10).std()\n",
    "data['Bollinger_Upper'] = rolling_mean + 2 * rolling_std\n",
    "data['Bollinger_Lower'] = rolling_mean - 2 * rolling_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding sharpe ration on 10 last days\n",
    "rolling_returns = data['Returns'].rolling(window=10)\n",
    "data['Sharpe_10'] = rolling_returns.mean() / rolling_returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the 10 first rows\n",
    "data = data.iloc[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500 Adj Close Price</th>\n",
       "      <th>Volume S&amp;P 500</th>\n",
       "      <th>Returns</th>\n",
       "      <th>VIX Adj Close Price</th>\n",
       "      <th>FED Rates</th>\n",
       "      <th>10 Y Treasury Rates</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Spread_10Y_Fed</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Bollinger_Upper</th>\n",
       "      <th>Bollinger_Lower</th>\n",
       "      <th>Sharpe_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-08</th>\n",
       "      <td>5995.540039</td>\n",
       "      <td>4666740000</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>14.94</td>\n",
       "      <td>4.423</td>\n",
       "      <td>4.306</td>\n",
       "      <td>3934.552132</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>5829.748975</td>\n",
       "      <td>5863.042240</td>\n",
       "      <td>6040.308074</td>\n",
       "      <td>5619.189875</td>\n",
       "      <td>0.286892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-11</th>\n",
       "      <td>6001.350098</td>\n",
       "      <td>4333000000</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>14.97</td>\n",
       "      <td>4.415</td>\n",
       "      <td>4.308</td>\n",
       "      <td>3935.952235</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>5847.531982</td>\n",
       "      <td>5888.189124</td>\n",
       "      <td>6084.175027</td>\n",
       "      <td>5610.888938</td>\n",
       "      <td>0.271452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-12</th>\n",
       "      <td>5983.990234</td>\n",
       "      <td>4243400000</td>\n",
       "      <td>-0.002893</td>\n",
       "      <td>14.71</td>\n",
       "      <td>4.423</td>\n",
       "      <td>4.432</td>\n",
       "      <td>3937.310040</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5862.639014</td>\n",
       "      <td>5905.607507</td>\n",
       "      <td>6113.968777</td>\n",
       "      <td>5611.309250</td>\n",
       "      <td>0.228504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-13</th>\n",
       "      <td>5985.379883</td>\n",
       "      <td>4220180000</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>14.02</td>\n",
       "      <td>4.390</td>\n",
       "      <td>4.451</td>\n",
       "      <td>3938.659546</td>\n",
       "      <td>0.061</td>\n",
       "      <td>5879.810010</td>\n",
       "      <td>5920.111576</td>\n",
       "      <td>6139.591012</td>\n",
       "      <td>5620.029008</td>\n",
       "      <td>0.262744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-14</th>\n",
       "      <td>5949.169922</td>\n",
       "      <td>4184570000</td>\n",
       "      <td>-0.006050</td>\n",
       "      <td>14.31</td>\n",
       "      <td>4.383</td>\n",
       "      <td>4.418</td>\n",
       "      <td>3939.972268</td>\n",
       "      <td>0.035</td>\n",
       "      <td>5904.181982</td>\n",
       "      <td>5925.394911</td>\n",
       "      <td>6135.423465</td>\n",
       "      <td>5672.940500</td>\n",
       "      <td>0.462806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            S&P 500 Adj Close Price  Volume S&P 500   Returns  \\\n",
       "Date                                                            \n",
       "2024-11-08              5995.540039      4666740000  0.003757   \n",
       "2024-11-11              6001.350098      4333000000  0.000969   \n",
       "2024-11-12              5983.990234      4243400000 -0.002893   \n",
       "2024-11-13              5985.379883      4220180000  0.000232   \n",
       "2024-11-14              5949.169922      4184570000 -0.006050   \n",
       "\n",
       "            VIX Adj Close Price  FED Rates  10 Y Treasury Rates         VWAP  \\\n",
       "Date                                                                           \n",
       "2024-11-08                14.94      4.423                4.306  3934.552132   \n",
       "2024-11-11                14.97      4.415                4.308  3935.952235   \n",
       "2024-11-12                14.71      4.423                4.432  3937.310040   \n",
       "2024-11-13                14.02      4.390                4.451  3938.659546   \n",
       "2024-11-14                14.31      4.383                4.418  3939.972268   \n",
       "\n",
       "            Spread_10Y_Fed       SMA_10       EMA_10  Bollinger_Upper  \\\n",
       "Date                                                                    \n",
       "2024-11-08          -0.117  5829.748975  5863.042240      6040.308074   \n",
       "2024-11-11          -0.107  5847.531982  5888.189124      6084.175027   \n",
       "2024-11-12           0.009  5862.639014  5905.607507      6113.968777   \n",
       "2024-11-13           0.061  5879.810010  5920.111576      6139.591012   \n",
       "2024-11-14           0.035  5904.181982  5925.394911      6135.423465   \n",
       "\n",
       "            Bollinger_Lower  Sharpe_10  \n",
       "Date                                    \n",
       "2024-11-08      5619.189875   0.286892  \n",
       "2024-11-11      5610.888938   0.271452  \n",
       "2024-11-12      5611.309250   0.228504  \n",
       "2024-11-13      5620.029008   0.262744  \n",
       "2024-11-14      5672.940500   0.462806  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display head of data\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 Adj Close Price    0\n",
      "Volume S&P 500             0\n",
      "Returns                    0\n",
      "VIX Adj Close Price        0\n",
      "FED Rates                  0\n",
      "10 Y Treasury Rates        0\n",
      "VWAP                       0\n",
      "Spread_10Y_Fed             0\n",
      "SMA_10                     0\n",
      "EMA_10                     0\n",
      "Bollinger_Upper            0\n",
      "Bollinger_Lower            0\n",
      "Sharpe_10                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for the NaN values in my dataset\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S&P 500 Adj Close Price    float64\n",
       "Volume S&P 500               int64\n",
       "Returns                    float64\n",
       "VIX Adj Close Price        float64\n",
       "FED Rates                  float64\n",
       "10 Y Treasury Rates        float64\n",
       "VWAP                       float64\n",
       "Spread_10Y_Fed             float64\n",
       "SMA_10                     float64\n",
       "EMA_10                     float64\n",
       "Bollinger_Upper            float64\n",
       "Bollinger_Lower            float64\n",
       "Sharpe_10                  float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand the nature of the data\n",
    "data.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to erase the time dependence and set the index as just a counter of rows\n",
    "data.reset_index(inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>S&amp;P 500 Adj Close Price</th>\n",
       "      <th>Volume S&amp;P 500</th>\n",
       "      <th>Returns</th>\n",
       "      <th>VIX Adj Close Price</th>\n",
       "      <th>FED Rates</th>\n",
       "      <th>10 Y Treasury Rates</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Spread_10Y_Fed</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Bollinger_Upper</th>\n",
       "      <th>Bollinger_Lower</th>\n",
       "      <th>Sharpe_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>2737.800049</td>\n",
       "      <td>3599200000</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>18.790001</td>\n",
       "      <td>2.310</td>\n",
       "      <td>3.035</td>\n",
       "      <td>2696.798141</td>\n",
       "      <td>0.725</td>\n",
       "      <td>2691.878979</td>\n",
       "      <td>2698.227137</td>\n",
       "      <td>2777.397385</td>\n",
       "      <td>2606.360574</td>\n",
       "      <td>0.107745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>2760.169922</td>\n",
       "      <td>4668310000</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>18.070000</td>\n",
       "      <td>2.308</td>\n",
       "      <td>3.013</td>\n",
       "      <td>2703.374130</td>\n",
       "      <td>0.705</td>\n",
       "      <td>2694.875977</td>\n",
       "      <td>2710.603192</td>\n",
       "      <td>2788.115169</td>\n",
       "      <td>2601.636784</td>\n",
       "      <td>0.090382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>2790.370117</td>\n",
       "      <td>4221580000</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>16.440001</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.992</td>\n",
       "      <td>2710.837355</td>\n",
       "      <td>0.704</td>\n",
       "      <td>2700.285986</td>\n",
       "      <td>2726.258987</td>\n",
       "      <td>2809.166069</td>\n",
       "      <td>2591.405904</td>\n",
       "      <td>0.153509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>2700.060059</td>\n",
       "      <td>4515710000</td>\n",
       "      <td>-0.032365</td>\n",
       "      <td>20.740000</td>\n",
       "      <td>2.365</td>\n",
       "      <td>2.924</td>\n",
       "      <td>2709.931499</td>\n",
       "      <td>0.559</td>\n",
       "      <td>2701.218994</td>\n",
       "      <td>2721.190192</td>\n",
       "      <td>2809.894846</td>\n",
       "      <td>2592.543142</td>\n",
       "      <td>0.028606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>2695.949951</td>\n",
       "      <td>5180430000</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>21.190001</td>\n",
       "      <td>2.353</td>\n",
       "      <td>2.876</td>\n",
       "      <td>2708.701895</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2706.625000</td>\n",
       "      <td>2716.363138</td>\n",
       "      <td>2807.265313</td>\n",
       "      <td>2605.984687</td>\n",
       "      <td>0.141898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  S&P 500 Adj Close Price  Volume S&P 500   Returns  \\\n",
       "0  2018-11-29              2737.800049      3599200000 -0.002183   \n",
       "1  2018-11-30              2760.169922      4668310000  0.008171   \n",
       "2  2018-12-03              2790.370117      4221580000  0.010941   \n",
       "3  2018-12-04              2700.060059      4515710000 -0.032365   \n",
       "4  2018-12-06              2695.949951      5180430000 -0.001522   \n",
       "\n",
       "   VIX Adj Close Price  FED Rates  10 Y Treasury Rates         VWAP  \\\n",
       "0            18.790001      2.310                3.035  2696.798141   \n",
       "1            18.070000      2.308                3.013  2703.374130   \n",
       "2            16.440001      2.288                2.992  2710.837355   \n",
       "3            20.740000      2.365                2.924  2709.931499   \n",
       "4            21.190001      2.353                2.876  2708.701895   \n",
       "\n",
       "   Spread_10Y_Fed       SMA_10       EMA_10  Bollinger_Upper  Bollinger_Lower  \\\n",
       "0           0.725  2691.878979  2698.227137      2777.397385      2606.360574   \n",
       "1           0.705  2694.875977  2710.603192      2788.115169      2601.636784   \n",
       "2           0.704  2700.285986  2726.258987      2809.166069      2591.405904   \n",
       "3           0.559  2701.218994  2721.190192      2809.894846      2592.543142   \n",
       "4           0.523  2706.625000  2716.363138      2807.265313      2605.984687   \n",
       "\n",
       "   Sharpe_10  \n",
       "0   0.107745  \n",
       "1   0.090382  \n",
       "2   0.153509  \n",
       "3   0.028606  \n",
       "4   0.141898  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed to implement our new categorical variables that relate to the dates\n",
    "\n",
    "import numpy as np\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding categorical variables based on dates (month, weekday, if it's a bank holiday) to break free from time dependency\n",
    "\n",
    "data['Month'] = pd.to_datetime(data['Date']).dt.month\n",
    "data['Weekday'] = pd.to_datetime(data['Date']).dt.dayofweek  # Monday=0, Sunday=6\n",
    "data['Is_Bank_Holiday'] = pd.to_datetime(data['Date']).isin(USFederalHolidayCalendar().holidays())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding categorical variable if it is a vacation\n",
    "\n",
    "data['Is_Vacation'] = pd.to_datetime(data['Date']).apply(\n",
    "    lambda x: (x.month == 12 and x.day >= 24) or (x.month == 1 and x.day <= 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting boolean values into numerical values\n",
    "\n",
    "data['Is_Bank_Holiday'] = data['Is_Bank_Holiday'].astype(int)\n",
    "data['Is_Vacation'] = data['Is_Vacation'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the date column to get rid of time dependency\n",
    "\n",
    "data.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500 Adj Close Price</th>\n",
       "      <th>Volume S&amp;P 500</th>\n",
       "      <th>Returns</th>\n",
       "      <th>VIX Adj Close Price</th>\n",
       "      <th>FED Rates</th>\n",
       "      <th>10 Y Treasury Rates</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Spread_10Y_Fed</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Bollinger_Upper</th>\n",
       "      <th>Bollinger_Lower</th>\n",
       "      <th>Sharpe_10</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Is_Bank_Holiday</th>\n",
       "      <th>Is_Vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2737.800049</td>\n",
       "      <td>3599200000</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>18.790001</td>\n",
       "      <td>2.310</td>\n",
       "      <td>3.035</td>\n",
       "      <td>2696.798141</td>\n",
       "      <td>0.725</td>\n",
       "      <td>2691.878979</td>\n",
       "      <td>2698.227137</td>\n",
       "      <td>2777.397385</td>\n",
       "      <td>2606.360574</td>\n",
       "      <td>0.107745</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2760.169922</td>\n",
       "      <td>4668310000</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>18.070000</td>\n",
       "      <td>2.308</td>\n",
       "      <td>3.013</td>\n",
       "      <td>2703.374130</td>\n",
       "      <td>0.705</td>\n",
       "      <td>2694.875977</td>\n",
       "      <td>2710.603192</td>\n",
       "      <td>2788.115169</td>\n",
       "      <td>2601.636784</td>\n",
       "      <td>0.090382</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2790.370117</td>\n",
       "      <td>4221580000</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>16.440001</td>\n",
       "      <td>2.288</td>\n",
       "      <td>2.992</td>\n",
       "      <td>2710.837355</td>\n",
       "      <td>0.704</td>\n",
       "      <td>2700.285986</td>\n",
       "      <td>2726.258987</td>\n",
       "      <td>2809.166069</td>\n",
       "      <td>2591.405904</td>\n",
       "      <td>0.153509</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2700.060059</td>\n",
       "      <td>4515710000</td>\n",
       "      <td>-0.032365</td>\n",
       "      <td>20.740000</td>\n",
       "      <td>2.365</td>\n",
       "      <td>2.924</td>\n",
       "      <td>2709.931499</td>\n",
       "      <td>0.559</td>\n",
       "      <td>2701.218994</td>\n",
       "      <td>2721.190192</td>\n",
       "      <td>2809.894846</td>\n",
       "      <td>2592.543142</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2695.949951</td>\n",
       "      <td>5180430000</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>21.190001</td>\n",
       "      <td>2.353</td>\n",
       "      <td>2.876</td>\n",
       "      <td>2708.701895</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2706.625000</td>\n",
       "      <td>2716.363138</td>\n",
       "      <td>2807.265313</td>\n",
       "      <td>2605.984687</td>\n",
       "      <td>0.141898</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2633.080078</td>\n",
       "      <td>4242240000</td>\n",
       "      <td>-0.023320</td>\n",
       "      <td>23.230000</td>\n",
       "      <td>2.340</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2703.621652</td>\n",
       "      <td>0.510</td>\n",
       "      <td>2704.940015</td>\n",
       "      <td>2700.584455</td>\n",
       "      <td>2810.254773</td>\n",
       "      <td>2599.625256</td>\n",
       "      <td>-0.029771</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2637.719971</td>\n",
       "      <td>4162880000</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>22.639999</td>\n",
       "      <td>2.330</td>\n",
       "      <td>2.856</td>\n",
       "      <td>2699.545909</td>\n",
       "      <td>0.526</td>\n",
       "      <td>2705.456006</td>\n",
       "      <td>2688.764552</td>\n",
       "      <td>2809.233996</td>\n",
       "      <td>2601.678016</td>\n",
       "      <td>0.019257</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2636.780029</td>\n",
       "      <td>3963440000</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>21.760000</td>\n",
       "      <td>2.370</td>\n",
       "      <td>2.879</td>\n",
       "      <td>2696.055596</td>\n",
       "      <td>0.509</td>\n",
       "      <td>2701.789014</td>\n",
       "      <td>2679.050584</td>\n",
       "      <td>2812.924055</td>\n",
       "      <td>2590.653973</td>\n",
       "      <td>-0.078867</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2651.070068</td>\n",
       "      <td>4029300000</td>\n",
       "      <td>0.005420</td>\n",
       "      <td>21.459999</td>\n",
       "      <td>2.375</td>\n",
       "      <td>2.906</td>\n",
       "      <td>2693.648529</td>\n",
       "      <td>0.531</td>\n",
       "      <td>2698.679028</td>\n",
       "      <td>2673.848312</td>\n",
       "      <td>2813.918926</td>\n",
       "      <td>2583.439131</td>\n",
       "      <td>-0.065067</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2650.540039</td>\n",
       "      <td>3978340000</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>20.650000</td>\n",
       "      <td>2.363</td>\n",
       "      <td>2.911</td>\n",
       "      <td>2691.485354</td>\n",
       "      <td>0.548</td>\n",
       "      <td>2689.354028</td>\n",
       "      <td>2669.532450</td>\n",
       "      <td>2803.455989</td>\n",
       "      <td>2575.252068</td>\n",
       "      <td>-0.244553</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2599.949951</td>\n",
       "      <td>4064370000</td>\n",
       "      <td>-0.019087</td>\n",
       "      <td>21.629999</td>\n",
       "      <td>2.368</td>\n",
       "      <td>2.891</td>\n",
       "      <td>2687.021631</td>\n",
       "      <td>0.523</td>\n",
       "      <td>2675.569019</td>\n",
       "      <td>2656.691218</td>\n",
       "      <td>2796.746824</td>\n",
       "      <td>2554.391213</td>\n",
       "      <td>-0.346005</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2545.939941</td>\n",
       "      <td>4661420000</td>\n",
       "      <td>-0.020773</td>\n",
       "      <td>24.520000</td>\n",
       "      <td>2.340</td>\n",
       "      <td>2.857</td>\n",
       "      <td>2679.549074</td>\n",
       "      <td>0.517</td>\n",
       "      <td>2654.146021</td>\n",
       "      <td>2636.308036</td>\n",
       "      <td>2784.267456</td>\n",
       "      <td>2524.024585</td>\n",
       "      <td>-0.545756</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2546.159912</td>\n",
       "      <td>4519190000</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>25.580000</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.825</td>\n",
       "      <td>2673.034078</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2629.725000</td>\n",
       "      <td>2619.753613</td>\n",
       "      <td>2735.631049</td>\n",
       "      <td>2523.818951</td>\n",
       "      <td>-0.676462</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2506.959961</td>\n",
       "      <td>5152830000</td>\n",
       "      <td>-0.015396</td>\n",
       "      <td>25.580000</td>\n",
       "      <td>2.338</td>\n",
       "      <td>2.778</td>\n",
       "      <td>2664.273267</td>\n",
       "      <td>0.440</td>\n",
       "      <td>2610.414990</td>\n",
       "      <td>2599.078240</td>\n",
       "      <td>2728.983630</td>\n",
       "      <td>2491.846351</td>\n",
       "      <td>-0.672072</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2467.419922</td>\n",
       "      <td>5619780000</td>\n",
       "      <td>-0.015772</td>\n",
       "      <td>28.379999</td>\n",
       "      <td>2.335</td>\n",
       "      <td>2.789</td>\n",
       "      <td>2653.563876</td>\n",
       "      <td>0.454</td>\n",
       "      <td>2587.561987</td>\n",
       "      <td>2574.980696</td>\n",
       "      <td>2720.127171</td>\n",
       "      <td>2454.996804</td>\n",
       "      <td>-0.796295</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2416.620117</td>\n",
       "      <td>7657890000</td>\n",
       "      <td>-0.020588</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>2.330</td>\n",
       "      <td>2.792</td>\n",
       "      <td>2637.210818</td>\n",
       "      <td>0.462</td>\n",
       "      <td>2565.915991</td>\n",
       "      <td>2546.030921</td>\n",
       "      <td>2731.920238</td>\n",
       "      <td>2399.911744</td>\n",
       "      <td>-0.798605</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2351.100098</td>\n",
       "      <td>2613670000</td>\n",
       "      <td>-0.027112</td>\n",
       "      <td>36.070000</td>\n",
       "      <td>2.333</td>\n",
       "      <td>2.749</td>\n",
       "      <td>2630.626390</td>\n",
       "      <td>0.416</td>\n",
       "      <td>2537.254004</td>\n",
       "      <td>2510.431049</td>\n",
       "      <td>2742.495470</td>\n",
       "      <td>2332.012537</td>\n",
       "      <td>-0.995544</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2467.699951</td>\n",
       "      <td>4249740000</td>\n",
       "      <td>0.049594</td>\n",
       "      <td>30.410000</td>\n",
       "      <td>2.380</td>\n",
       "      <td>2.797</td>\n",
       "      <td>2624.749701</td>\n",
       "      <td>0.417</td>\n",
       "      <td>2520.345996</td>\n",
       "      <td>2502.633460</td>\n",
       "      <td>2716.817835</td>\n",
       "      <td>2323.874157</td>\n",
       "      <td>-0.284752</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2488.830078</td>\n",
       "      <td>4139010000</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>29.959999</td>\n",
       "      <td>2.348</td>\n",
       "      <td>2.743</td>\n",
       "      <td>2620.136922</td>\n",
       "      <td>0.395</td>\n",
       "      <td>2504.121997</td>\n",
       "      <td>2500.116280</td>\n",
       "      <td>2678.127044</td>\n",
       "      <td>2330.116950</td>\n",
       "      <td>-0.268276</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2485.739990</td>\n",
       "      <td>3728440000</td>\n",
       "      <td>-0.001242</td>\n",
       "      <td>28.340000</td>\n",
       "      <td>2.328</td>\n",
       "      <td>2.736</td>\n",
       "      <td>2616.150139</td>\n",
       "      <td>0.408</td>\n",
       "      <td>2487.641992</td>\n",
       "      <td>2497.496044</td>\n",
       "      <td>2627.972873</td>\n",
       "      <td>2347.311111</td>\n",
       "      <td>-0.273215</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    S&P 500 Adj Close Price  Volume S&P 500   Returns  VIX Adj Close Price  \\\n",
       "0               2737.800049      3599200000 -0.002183            18.790001   \n",
       "1               2760.169922      4668310000  0.008171            18.070000   \n",
       "2               2790.370117      4221580000  0.010941            16.440001   \n",
       "3               2700.060059      4515710000 -0.032365            20.740000   \n",
       "4               2695.949951      5180430000 -0.001522            21.190001   \n",
       "5               2633.080078      4242240000 -0.023320            23.230000   \n",
       "6               2637.719971      4162880000  0.001762            22.639999   \n",
       "7               2636.780029      3963440000 -0.000356            21.760000   \n",
       "8               2651.070068      4029300000  0.005420            21.459999   \n",
       "9               2650.540039      3978340000 -0.000200            20.650000   \n",
       "10              2599.949951      4064370000 -0.019087            21.629999   \n",
       "11              2545.939941      4661420000 -0.020773            24.520000   \n",
       "12              2546.159912      4519190000  0.000086            25.580000   \n",
       "13              2506.959961      5152830000 -0.015396            25.580000   \n",
       "14              2467.419922      5619780000 -0.015772            28.379999   \n",
       "15              2416.620117      7657890000 -0.020588            30.110001   \n",
       "16              2351.100098      2613670000 -0.027112            36.070000   \n",
       "17              2467.699951      4249740000  0.049594            30.410000   \n",
       "18              2488.830078      4139010000  0.008563            29.959999   \n",
       "19              2485.739990      3728440000 -0.001242            28.340000   \n",
       "\n",
       "    FED Rates  10 Y Treasury Rates         VWAP  Spread_10Y_Fed       SMA_10  \\\n",
       "0       2.310                3.035  2696.798141           0.725  2691.878979   \n",
       "1       2.308                3.013  2703.374130           0.705  2694.875977   \n",
       "2       2.288                2.992  2710.837355           0.704  2700.285986   \n",
       "3       2.365                2.924  2709.931499           0.559  2701.218994   \n",
       "4       2.353                2.876  2708.701895           0.523  2706.625000   \n",
       "5       2.340                2.850  2703.621652           0.510  2704.940015   \n",
       "6       2.330                2.856  2699.545909           0.526  2705.456006   \n",
       "7       2.370                2.879  2696.055596           0.509  2701.789014   \n",
       "8       2.375                2.906  2693.648529           0.531  2698.679028   \n",
       "9       2.363                2.911  2691.485354           0.548  2689.354028   \n",
       "10      2.368                2.891  2687.021631           0.523  2675.569019   \n",
       "11      2.340                2.857  2679.549074           0.517  2654.146021   \n",
       "12      2.328                2.825  2673.034078           0.497  2629.725000   \n",
       "13      2.338                2.778  2664.273267           0.440  2610.414990   \n",
       "14      2.335                2.789  2653.563876           0.454  2587.561987   \n",
       "15      2.330                2.792  2637.210818           0.462  2565.915991   \n",
       "16      2.333                2.749  2630.626390           0.416  2537.254004   \n",
       "17      2.380                2.797  2624.749701           0.417  2520.345996   \n",
       "18      2.348                2.743  2620.136922           0.395  2504.121997   \n",
       "19      2.328                2.736  2616.150139           0.408  2487.641992   \n",
       "\n",
       "         EMA_10  Bollinger_Upper  Bollinger_Lower  Sharpe_10  Month  Weekday  \\\n",
       "0   2698.227137      2777.397385      2606.360574   0.107745     11        3   \n",
       "1   2710.603192      2788.115169      2601.636784   0.090382     11        4   \n",
       "2   2726.258987      2809.166069      2591.405904   0.153509     12        0   \n",
       "3   2721.190192      2809.894846      2592.543142   0.028606     12        1   \n",
       "4   2716.363138      2807.265313      2605.984687   0.141898     12        3   \n",
       "5   2700.584455      2810.254773      2599.625256  -0.029771     12        4   \n",
       "6   2688.764552      2809.233996      2601.678016   0.019257     12        0   \n",
       "7   2679.050584      2812.924055      2590.653973  -0.078867     12        1   \n",
       "8   2673.848312      2813.918926      2583.439131  -0.065067     12        2   \n",
       "9   2669.532450      2803.455989      2575.252068  -0.244553     12        3   \n",
       "10  2656.691218      2796.746824      2554.391213  -0.346005     12        4   \n",
       "11  2636.308036      2784.267456      2524.024585  -0.545756     12        0   \n",
       "12  2619.753613      2735.631049      2523.818951  -0.676462     12        1   \n",
       "13  2599.078240      2728.983630      2491.846351  -0.672072     12        2   \n",
       "14  2574.980696      2720.127171      2454.996804  -0.796295     12        3   \n",
       "15  2546.030921      2731.920238      2399.911744  -0.798605     12        4   \n",
       "16  2510.431049      2742.495470      2332.012537  -0.995544     12        0   \n",
       "17  2502.633460      2716.817835      2323.874157  -0.284752     12        2   \n",
       "18  2500.116280      2678.127044      2330.116950  -0.268276     12        3   \n",
       "19  2497.496044      2627.972873      2347.311111  -0.273215     12        4   \n",
       "\n",
       "    Is_Bank_Holiday  Is_Vacation  \n",
       "0                 0            0  \n",
       "1                 0            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 0            0  \n",
       "5                 0            0  \n",
       "6                 0            0  \n",
       "7                 0            0  \n",
       "8                 0            0  \n",
       "9                 0            0  \n",
       "10                0            0  \n",
       "11                0            0  \n",
       "12                0            0  \n",
       "13                0            0  \n",
       "14                0            0  \n",
       "15                0            0  \n",
       "16                0            1  \n",
       "17                0            1  \n",
       "18                0            1  \n",
       "19                0            1  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P 500 Adj Close Price    0\n",
      "Volume S&P 500             0\n",
      "Returns                    0\n",
      "VIX Adj Close Price        0\n",
      "FED Rates                  0\n",
      "10 Y Treasury Rates        0\n",
      "VWAP                       0\n",
      "Spread_10Y_Fed             0\n",
      "SMA_10                     0\n",
      "EMA_10                     0\n",
      "Bollinger_Upper            0\n",
      "Bollinger_Lower            0\n",
      "Sharpe_10                  0\n",
      "Month                      0\n",
      "Weekday                    0\n",
      "Is_Bank_Holiday            0\n",
      "Is_Vacation                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for NaN values\n",
    "\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library to standardize variables\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to standardize\n",
    "\n",
    "cols_to_standardize = [\n",
    "    \"S&P 500 Adj Close Price\",\n",
    "    \"Volume S&P 500\",\n",
    "    \"VIX Adj Close Price\",\n",
    "    \"FED Rates\",\n",
    "    \"10 Y Treasury Rates\",\n",
    "    \"VWAP\",\n",
    "    \"Spread_10Y_Fed\",\n",
    "    \"SMA_10\",\n",
    "    \"EMA_10\",\n",
    "    \"Bollinger_Upper\",\n",
    "    \"Bollinger_Lower\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler initialization\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying standardization to my dataset\n",
    "\n",
    "data[cols_to_standardize] = scaler.fit_transform(data[cols_to_standardize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500 Adj Close Price</th>\n",
       "      <th>Volume S&amp;P 500</th>\n",
       "      <th>Returns</th>\n",
       "      <th>VIX Adj Close Price</th>\n",
       "      <th>FED Rates</th>\n",
       "      <th>10 Y Treasury Rates</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Spread_10Y_Fed</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>Bollinger_Upper</th>\n",
       "      <th>Bollinger_Lower</th>\n",
       "      <th>Sharpe_10</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Is_Bank_Holiday</th>\n",
       "      <th>Is_Vacation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.480252</td>\n",
       "      <td>-0.632549</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>-0.222355</td>\n",
       "      <td>-0.022333</td>\n",
       "      <td>0.369660</td>\n",
       "      <td>-1.526346</td>\n",
       "      <td>0.522385</td>\n",
       "      <td>-1.534519</td>\n",
       "      <td>-1.528235</td>\n",
       "      <td>-1.539739</td>\n",
       "      <td>-1.521039</td>\n",
       "      <td>0.107745</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.453469</td>\n",
       "      <td>0.409573</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>-0.312870</td>\n",
       "      <td>-0.023292</td>\n",
       "      <td>0.352474</td>\n",
       "      <td>-1.509770</td>\n",
       "      <td>0.502284</td>\n",
       "      <td>-1.530905</td>\n",
       "      <td>-1.513299</td>\n",
       "      <td>-1.526914</td>\n",
       "      <td>-1.526750</td>\n",
       "      <td>0.090382</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.417311</td>\n",
       "      <td>-0.025880</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>-0.517785</td>\n",
       "      <td>-0.032880</td>\n",
       "      <td>0.336069</td>\n",
       "      <td>-1.490957</td>\n",
       "      <td>0.501279</td>\n",
       "      <td>-1.524381</td>\n",
       "      <td>-1.494403</td>\n",
       "      <td>-1.501723</td>\n",
       "      <td>-1.539119</td>\n",
       "      <td>0.153509</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.525436</td>\n",
       "      <td>0.260825</td>\n",
       "      <td>-0.032365</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.282946</td>\n",
       "      <td>-1.493240</td>\n",
       "      <td>0.355553</td>\n",
       "      <td>-1.523256</td>\n",
       "      <td>-1.500521</td>\n",
       "      <td>-1.500851</td>\n",
       "      <td>-1.537744</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.530357</td>\n",
       "      <td>0.908766</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>0.079362</td>\n",
       "      <td>-0.001721</td>\n",
       "      <td>0.245448</td>\n",
       "      <td>-1.496340</td>\n",
       "      <td>0.319372</td>\n",
       "      <td>-1.516736</td>\n",
       "      <td>-1.506347</td>\n",
       "      <td>-1.503998</td>\n",
       "      <td>-1.521494</td>\n",
       "      <td>0.141898</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S&P 500 Adj Close Price  Volume S&P 500   Returns  VIX Adj Close Price  \\\n",
       "0                -1.480252       -0.632549 -0.002183            -0.222355   \n",
       "1                -1.453469        0.409573  0.008171            -0.312870   \n",
       "2                -1.417311       -0.025880  0.010941            -0.517785   \n",
       "3                -1.525436        0.260825 -0.032365             0.022790   \n",
       "4                -1.530357        0.908766 -0.001522             0.079362   \n",
       "\n",
       "   FED Rates  10 Y Treasury Rates      VWAP  Spread_10Y_Fed    SMA_10  \\\n",
       "0  -0.022333             0.369660 -1.526346        0.522385 -1.534519   \n",
       "1  -0.023292             0.352474 -1.509770        0.502284 -1.530905   \n",
       "2  -0.032880             0.336069 -1.490957        0.501279 -1.524381   \n",
       "3   0.004032             0.282946 -1.493240        0.355553 -1.523256   \n",
       "4  -0.001721             0.245448 -1.496340        0.319372 -1.516736   \n",
       "\n",
       "     EMA_10  Bollinger_Upper  Bollinger_Lower  Sharpe_10  Month  Weekday  \\\n",
       "0 -1.528235        -1.539739        -1.521039   0.107745     11        3   \n",
       "1 -1.513299        -1.526914        -1.526750   0.090382     11        4   \n",
       "2 -1.494403        -1.501723        -1.539119   0.153509     12        0   \n",
       "3 -1.500521        -1.500851        -1.537744   0.028606     12        1   \n",
       "4 -1.506347        -1.503998        -1.521494   0.141898     12        3   \n",
       "\n",
       "   Is_Bank_Holiday  Is_Vacation  \n",
       "0                0            0  \n",
       "1                0            0  \n",
       "2                0            0  \n",
       "3                0            0  \n",
       "4                0            0  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing useful libraries to perform Random Forest Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing variables for model\n",
    "\n",
    "X = data.drop(columns=[\"S&P 500 Adj Close Price\"])  \n",
    "y = data[\"S&P 500 Adj Close Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing random forest model\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediciting on test variable\n",
    "\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.03895279653506159\n",
      "R²: 0.9983869916166143\n"
     ]
    }
   ],
   "source": [
    "# evaluating performances\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions :\n",
      "True : 0.17361314504924205, Predicted : 0.17881384618901652\n",
      "True : 1.484509878297498, Predicted : 1.4599183054522715\n",
      "True : -0.773901626599777, Predicted : -0.8144641105139386\n",
      "True : -0.884181760754375, Predicted : -0.8688431665335595\n",
      "True : -0.7047955720015995, Predicted : -0.7233695622686713\n"
     ]
    }
   ],
   "source": [
    "# example of prediction\n",
    "\n",
    "print(\"Some predictions :\")\n",
    "for i in range(5):\n",
    "    print(f\"True : {y_test.iloc[i]}, Predicted : {y_pred[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries to compute good regression metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.03895279653506159\n",
      "MAE: 0.026807339558940435\n",
      "R²: 0.9983869916166143\n",
      "Average of the residual Z-scores: -1.1842378929335004e-17\n",
      "Standard error the the residual Z-scores: 1.0\n"
     ]
    }
   ],
   "source": [
    "# computing right metrics to evaluate the predictions and thus the model\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "z_scores = stats.zscore(residuals)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"R²: {r2}\")\n",
    "print(f\"Average of the residual Z-scores: {np.mean(z_scores)}\")\n",
    "print(f\"Standard error the the residual Z-scores: {np.std(z_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding what we're doing, criticizing our job and then trying to define what we're going to do next.\n",
    "\n",
    "Here our RandomForestRegression Model provides a very high R² which leads me to worry about overfitting so I could think of testing it on another dataset or even think about some methods like Ridge or Lasso.\n",
    "\n",
    "Also, my model assumes that the relationships between features are constant in time which is a strong assumption and could be solved by using other variables or models like neural network.\n",
    "\n",
    "Moreover, extrapolation could be difficult because the model focuses on past data but it is norma since it is the only data we can base ourselves on. One solution could be to test the model on realistic future periods or even use more dynamic models that ajust the model parameters with newer data.\n",
    "\n",
    "Talking about optimizing the parameters, we're now going to try and use GridCV to think about and try to choose the right parameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters : {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best score : 0.011880130561495731\n"
     ]
    }
   ],
   "source": [
    "# trying to find out what the best parameters are for our random forest regression model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters : {grid_search.best_params_}\")\n",
    "print(f\"Best score : {-grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's think about the core strategy of my project. \n",
    "\n",
    "Here I am trying to predict the future price of the S&P 500 index. Here Thus, I've used a Random Forest which is a model that we've studied in class but here I used the regression way instead of the Random Forest Classification because what I have here is a regression problem becuse of what I'm trying to predict. \n",
    "\n",
    "An idea would be to try to think about implementing other models we've seen in class by changing the aim of the project and turning it into a classification problem, at leat for the sake of the stage 2, in order to use models we've studied in class. Here, what I would be trying to predict would be if the S&P 500 stock woul rather go up or down. \n",
    "\n",
    "In further stages I'll then be able to comeback to more difficult and deep regression models such as SVMs, XGBoost... by coming back to my initial regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, as mentioned above I'm going to change my regression into a classification problem and try to implement first a simple model such as Logistic Regression to see the outputs and results I get to see what I'll try to do next to enhance my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a categorical variable to tell if the price of the stock will go up or down\n",
    "data['Target'] = (data['Returns'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "1    817\n",
      "0    683\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# checking the class distribution\n",
    "print(data['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the variables for the model implementation\n",
    "features = [\n",
    "    'S&P 500 Adj Close Price', 'Volume S&P 500', 'VIX Adj Close Price',\n",
    "    'FED Rates', '10 Y Treasury Rates', 'VWAP', 'Spread_10Y_Fed',\n",
    "    'SMA_10', 'EMA_10', 'Bollinger_Upper', 'Bollinger_Lower',\n",
    "    'Sharpe_10', 'Month', 'Weekday', 'Is_Bank_Holiday', 'Is_Vacation'\n",
    "]\n",
    "X = data[features]\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the model and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.62\n",
      "Logistic Regression Precision: 0.6129032258064516\n",
      "Logistic Regression Recall: 0.7307692307692307\n",
      "Logistic Regression F1 Score: 0.6666666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.50      0.56       144\n",
      "           1       0.61      0.73      0.67       156\n",
      "\n",
      "    accuracy                           0.62       300\n",
      "   macro avg       0.62      0.62      0.61       300\n",
      "weighted avg       0.62      0.62      0.61       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# implementing the model and printing the results\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Logistic Regression Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Logistic Regression Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"Logistic Regression F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so here we printed the results of our first model after having transformed it into a classification problem. The model used here was a rather simple one to begin. I chose to use a Logistic Regression model which assumes linear relationship between features and the probability to belong to a certain class. The problem here is that the model doesn't catch the complex relationships between features. We can easily see it by looking at the accuracy score that there is a lot of room for improvement. The precision score shows that the model is good for the positive class (price rises) but struggles predicting for the negative class surely because the model doesn't capture complex enough relationships. The low recall score also points out that the model still fails to miss some elements from the positive class. Thus, I will try to enhance my results by implementing a bagging and then a boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.44      0.50       144\n",
      "           1       0.58      0.72      0.64       156\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.58      0.58      0.57       300\n",
      "weighted avg       0.58      0.58      0.57       300\n",
      "\n",
      "Bagging Accuracy: 0.5833333333333334\n",
      "Bagging Precision: 0.5803108808290155\n",
      "Bagging Recall: 0.717948717948718\n",
      "Bagging F1 Score: 0.6418338108882522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_bagging))\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bagging))\n",
    "print(\"Bagging Precision:\", precision_score(y_test, y_pred_bagging))\n",
    "print(\"Bagging Recall:\", recall_score(y_test, y_pred_bagging))\n",
    "print(\"Bagging F1 Score:\", f1_score(y_test, y_pred_bagging))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we understand that the bagging model, even though it is able to capture more complex relationships, seems less appropriate to this dataset. Here the problem might come from an imbalance between classes, maybe because of trees not deep enough. One issue might also be overfitting and that would be why Logistic Regression performs better because it doesn't overadjust sub-samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.47      0.53       144\n",
      "           1       0.60      0.74      0.66       156\n",
      "\n",
      "    accuracy                           0.61       300\n",
      "   macro avg       0.61      0.60      0.60       300\n",
      "weighted avg       0.61      0.61      0.60       300\n",
      "\n",
      "Boosting Accuracy: 0.6066666666666667\n",
      "Boosting Precision: 0.5989583333333334\n",
      "Boosting Recall: 0.7371794871794872\n",
      "Boosting F1 Score: 0.6609195402298851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "boosting_model = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_boosting = boosting_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_boosting))\n",
    "print(\"Boosting Accuracy:\", accuracy_score(y_test, y_pred_boosting))\n",
    "print(\"Boosting Precision:\", precision_score(y_test, y_pred_boosting))\n",
    "print(\"Boosting Recall:\", recall_score(y_test, y_pred_boosting))\n",
    "print(\"Boosting F1 Score:\", f1_score(y_test, y_pred_boosting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so let's try to understand why the boosting model isn't capturing well the relationships between features. Among those models, Logistic Regression is the more adapted but is limited for the minority class. The problem of bagging is that it doesn't provide any significative improvement and is less performing for the minority class. The boosting model shows clear improvements for the minority class thanks to the bias and error reduction provided by this type of model. One idea for the next improvements would be to try to improve the boosting parameters and hyperparameters to see how the results evolve. Another idea would be to try to improve the Logistic Regression by also changing the parameters and adjsuting the weights of the under represented class. Also, I could also think of implementing other advanced models like XGBoost or GradientBoosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first idea to increase my scores for the models is to increase the number of rows in my datasets. Until now, I have been using the S&P 500 data from 1 year ago making my dataset only have 253 rows. I've now understood that my models might perform better if I have around 1000 rows for my models to be trained on richer data. Thus, I will take the S&P 500 data from 2018 to see how the model evolves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand and criticize the results of each model using the S&P 500 data from 5 years ago until this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Logistic Regression still remains the best performing over the other models like AdaBoost, Bagging and Boosting methods.\n",
    "Let's try to understand where the issue might come from because our models' accuracy is still low even with a bigger numer of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution :\n",
      " Target\n",
      "1    817\n",
      "0    683\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class proportion :\n",
      " Target\n",
      "1    0.544667\n",
      "0    0.455333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Class counting\n",
    "class_counts = data['Target'].value_counts()\n",
    "print(\"Class distribution :\\n\", class_counts)\n",
    "\n",
    "# Class proportion\n",
    "class_proportions = data['Target'].value_counts(normalize=True)\n",
    "print(\"\\nClass proportion :\\n\", class_proportions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the classes are relatively well balanced. We can understand well here why Logistic Regression works well an is an appropriate model for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try here to implement a more advanced model which is XGBoost in order to see if it can correct the bagging and boosting limitations by capturing better the complex relationships between the features of my dataset. We will also try to determine the right hyperparameters to reduce the variance and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.49      0.55       144\n",
      "           1       0.61      0.74      0.67       156\n",
      "\n",
      "    accuracy                           0.62       300\n",
      "   macro avg       0.62      0.61      0.61       300\n",
      "weighted avg       0.62      0.62      0.61       300\n",
      "\n",
      "XGBoosting Accuracy: 0.6166666666666667\n",
      "XGBoosting Precision: 0.6084656084656085\n",
      "XGBoosting Recall: 0.7371794871794872\n",
      "XGBoosting F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# implementing and testing the XG Boost Model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(\"XGBoosting Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoosting Precision:\", precision_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoosting Recall:\", recall_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoosting F1 Score:\", f1_score(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the XGBoost we have a better recall than for other models, but it goes with the decrease of accuracy as we've seen in class it depends on the Machine Learning problem we are dealing with. Here we would like to have a higher accuracy than recall because in Time Series Forecasting questions, having a low recall isn't a huge problem compared to Health and Cancer models for example, because having a false negative in Time Series corresponds to predicting that the price goes down but in reality it went up, which is way less a problem than for cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50, 'scale_pos_weight': 1}\n",
      "Optimized Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.31      0.41       144\n",
      "           1       0.56      0.83      0.67       156\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.59      0.57      0.54       300\n",
      "weighted avg       0.59      0.58      0.54       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:30:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameters list\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'scale_pos_weight': [1, 2, 5]  \n",
    "}\n",
    "\n",
    "# GridSearch Initialization\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_optimized = best_model.predict(X_test)\n",
    "print(\"Optimized Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.31      0.41       144\n",
      "           1       0.56      0.83      0.67       156\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.59      0.57      0.54       300\n",
      "weighted avg       0.59      0.58      0.54       300\n",
      "\n",
      "XGBoosting Accuracy: 0.5766666666666667\n",
      "XGBoosting Precision: 0.5633187772925764\n",
      "XGBoosting Recall: 0.8269230769230769\n",
      "XGBoosting F1 Score: 0.6701298701298701\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_optimized))\n",
    "print(\"XGBoosting Accuracy:\", accuracy_score(y_test, y_pred_optimized))\n",
    "print(\"XGBoosting Precision:\", precision_score(y_test, y_pred_optimized))\n",
    "print(\"XGBoosting Recall:\", recall_score(y_test, y_pred_optimized))\n",
    "print(\"XGBoosting F1 Score:\", f1_score(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, optimizing the hyperparameters of the XGBoost model has increased a lot the recall which would be great in another application but here the problem is the accuracy which has decreased with the parameters optimization. Thus, what I will do now is try to optimize the parameters and hyperparameters for the Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Meilleurs paramètres : {'C': 100, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Meilleure accuracy (CV) : 0.7233333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définir la grille d’hyperparamètres\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],            # Régularisation\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'], # Optimiseurs\n",
    "    'penalty': ['l2'],                       # Type de pénalité (l2 est le plus commun)\n",
    "    'class_weight': [None, 'balanced']       # Gestion du déséquilibre\n",
    "}\n",
    "\n",
    "# Modèle Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Recherche par validation croisée\n",
    "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleurs paramètres\n",
    "print(\"Meilleurs paramètres :\", grid_search.best_params_)\n",
    "\n",
    "# Meilleure accuracy\n",
    "print(\"Meilleure accuracy (CV) :\", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.56      0.63       144\n",
      "           1       0.66      0.80      0.72       156\n",
      "\n",
      "    accuracy                           0.68       300\n",
      "   macro avg       0.69      0.68      0.68       300\n",
      "weighted avg       0.69      0.68      0.68       300\n",
      "\n",
      "Logistic Regression Optimized Accuracy: 0.6833333333333333\n",
      "Logistic Regression Optimized Precision: 0.6613756613756614\n",
      "Logistic Regression Optimized Recall: 0.8012820512820513\n",
      "Logistic Regression Optimized F1 Score: 0.7246376811594203\n"
     ]
    }
   ],
   "source": [
    "# Take the best model and train/test the optimized model\n",
    "\n",
    "best_logreg = grid_search.best_estimator_\n",
    "\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_logopti = best_logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_logopti))\n",
    "print(\"Logistic Regression Optimized Accuracy:\", accuracy_score(y_test, y_pred_logopti))\n",
    "print(\"Logistic Regression Optimized Precision:\", precision_score(y_test, y_pred_logopti))\n",
    "print(\"Logistic Regression Optimized Recall:\", recall_score(y_test, y_pred_logopti))\n",
    "print(\"Logistic Regression Optimized F1 Score:\", f1_score(y_test, y_pred_logopti))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we have a good increase in accuracy compared to all my previous models, but also in the recall which is a good improvement compared to the Logistic Regression ran without the optimized hyperparameters. Let's try to understand and explain the reasons why Optimized Logistic Regression provides the best results for accuracy. Here, the dataset I'm using might have some linear relationships that the LogisticRegression model captures easily. Also, optimizing the hyperparameters allowed to reduce significantly overfitting (what we've seen with the F-1 score before and after hyperparameter optimization) and enhancing generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I will now try to do is to continue optimizing my Logistic Regression Model and last I will try to implement a model that's outside the scope of this course that could be a Deep Learning Model such as Neuronal Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des classes avant SMOTE :\n",
      "Target\n",
      "1    661\n",
      "0    539\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution des classes après SMOTE :\n",
      "Target\n",
      "0    661\n",
      "1    661\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using SMOTE to rebalance classes\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Distribution des classes avant SMOTE :\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nDistribution des classes après SMOTE :\")\n",
    "print(y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       144\n",
      "           1       0.71      0.72      0.72       156\n",
      "\n",
      "    accuracy                           0.70       300\n",
      "   macro avg       0.70      0.70      0.70       300\n",
      "weighted avg       0.70      0.70      0.70       300\n",
      "\n",
      "Logistic Regression Optimized Smote Accuracy: 0.7\n",
      "Logistic Regression Optimized SmotePrecision: 0.70625\n",
      "Logistic Regression Optimized Smote Recall: 0.7243589743589743\n",
      "Logistic Regression Optimized Smote F1 Score: 0.7151898734177216\n"
     ]
    }
   ],
   "source": [
    "# testing the optimized model after SMOTE\n",
    "\n",
    "best_logreg.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "y_pred_logoptismote = best_logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred_logoptismote))\n",
    "print(\"Logistic Regression Optimized Smote Accuracy:\", accuracy_score(y_test, y_pred_logoptismote))\n",
    "print(\"Logistic Regression Optimized SmotePrecision:\", precision_score(y_test, y_pred_logoptismote))\n",
    "print(\"Logistic Regression Optimized Smote Recall:\", recall_score(y_test, y_pred_logoptismote))\n",
    "print(\"Logistic Regression Optimized Smote F1 Score:\", f1_score(y_test, y_pred_logoptismote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I used SMOTE to rebalance classes and managed to increase the accuracy of my model to 0.7 but this was possible by reducing the recall score which is not a huge problem as I explained before. I'm going to try to keep optimizing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67       144\n",
      "           1       0.69      0.69      0.69       156\n",
      "\n",
      "    accuracy                           0.68       300\n",
      "   macro avg       0.68      0.68      0.68       300\n",
      "weighted avg       0.68      0.68      0.68       300\n",
      "\n",
      "Logistic Regression Optimized Smote elsaticnet Accuracy: 0.68\n",
      "Logistic Regression Optimized Smote elasticnet Precision: 0.6948051948051948\n",
      "Logistic Regression Optimized Smote elasticnet Recall: 0.6858974358974359\n",
      "Logistic Regression Optimized Smote elasticnet F1 Score: 0.6903225806451613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizing hyperparameters using elasticnet\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],          \n",
    "    'l1_ratio': [0.1, 0.5, 0.9],            \n",
    "    'solver': ['saga'],                     \n",
    "    'penalty': ['elasticnet']               \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=best_logreg, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_best_model = best_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_best_model))\n",
    "print(\"Logistic Regression Optimized Smote elsaticnet Accuracy:\", accuracy_score(y_test, y_pred_best_model))\n",
    "print(\"Logistic Regression Optimized Smote elasticnet Precision:\", precision_score(y_test, y_pred_best_model))\n",
    "print(\"Logistic Regression Optimized Smote elasticnet Recall:\", recall_score(y_test, y_pred_best_model))\n",
    "print(\"Logistic Regression Optimized Smote elasticnet F1 Score:\", f1_score(y_test, y_pred_best_model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Elasticnet to optimize even more the parameters was not the best solution because it reduced the accuracy of the model. This might be linked to the previous of SMOTE that can make Elasticnet's regularisation less effective because it might try to adjust artificial parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having optimized a lot my Logistic Regression, I will try to go further and implement a Deep Learning algorithm which is Neural Network algorithm to see how it compares to my Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyama\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.5432 - loss: 0.7411 - val_accuracy: 0.5800 - val_loss: 0.6700\n",
      "Epoch 2/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5819 - loss: 0.6788 - val_accuracy: 0.5833 - val_loss: 0.6634\n",
      "Epoch 3/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6025 - loss: 0.6563 - val_accuracy: 0.5900 - val_loss: 0.6625\n",
      "Epoch 4/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6350 - loss: 0.6534 - val_accuracy: 0.5967 - val_loss: 0.6606\n",
      "Epoch 5/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6186 - loss: 0.6432 - val_accuracy: 0.5833 - val_loss: 0.6603\n",
      "Epoch 6/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6347 - loss: 0.6442 - val_accuracy: 0.5867 - val_loss: 0.6583\n",
      "Epoch 7/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6288 - loss: 0.6422 - val_accuracy: 0.5800 - val_loss: 0.6590\n",
      "Epoch 8/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6084 - loss: 0.6516 - val_accuracy: 0.5667 - val_loss: 0.6594\n",
      "Epoch 9/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6148 - loss: 0.6524 - val_accuracy: 0.5700 - val_loss: 0.6588\n",
      "Epoch 10/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6163 - loss: 0.6487 - val_accuracy: 0.5633 - val_loss: 0.6591\n",
      "Epoch 11/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6222 - loss: 0.6511 - val_accuracy: 0.5700 - val_loss: 0.6577\n",
      "Epoch 12/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6214 - loss: 0.6419 - val_accuracy: 0.5633 - val_loss: 0.6575\n",
      "Epoch 13/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6366 - loss: 0.6458 - val_accuracy: 0.5667 - val_loss: 0.6577\n",
      "Epoch 14/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6123 - loss: 0.6483 - val_accuracy: 0.5600 - val_loss: 0.6587\n",
      "Epoch 15/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6424 - loss: 0.6343 - val_accuracy: 0.5733 - val_loss: 0.6588\n",
      "Epoch 16/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6433 - loss: 0.6329 - val_accuracy: 0.5600 - val_loss: 0.6584\n",
      "Epoch 17/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6427 - loss: 0.6424 - val_accuracy: 0.5533 - val_loss: 0.6582\n",
      "Epoch 18/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6472 - loss: 0.6294 - val_accuracy: 0.5533 - val_loss: 0.6597\n",
      "Epoch 19/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6332 - loss: 0.6217 - val_accuracy: 0.5633 - val_loss: 0.6583\n",
      "Epoch 20/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6488 - loss: 0.6332 - val_accuracy: 0.5533 - val_loss: 0.6567\n",
      "Epoch 21/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6528 - loss: 0.6184 - val_accuracy: 0.5633 - val_loss: 0.6582\n",
      "Epoch 22/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6508 - loss: 0.6284 - val_accuracy: 0.5600 - val_loss: 0.6574\n",
      "Epoch 23/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6323 - loss: 0.6277 - val_accuracy: 0.5567 - val_loss: 0.6565\n",
      "Epoch 24/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6301 - loss: 0.6362 - val_accuracy: 0.5633 - val_loss: 0.6595\n",
      "Epoch 25/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6370 - loss: 0.6302 - val_accuracy: 0.5567 - val_loss: 0.6583\n",
      "Epoch 26/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6223 - loss: 0.6365 - val_accuracy: 0.5500 - val_loss: 0.6566\n",
      "Epoch 27/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6532 - loss: 0.6223 - val_accuracy: 0.5467 - val_loss: 0.6582\n",
      "Epoch 28/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6364 - loss: 0.6325 - val_accuracy: 0.5633 - val_loss: 0.6570\n",
      "Epoch 29/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6327 - loss: 0.6180 - val_accuracy: 0.5533 - val_loss: 0.6580\n",
      "Epoch 30/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6258 - loss: 0.6325 - val_accuracy: 0.5700 - val_loss: 0.6596\n",
      "Epoch 31/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6369 - loss: 0.6326 - val_accuracy: 0.5633 - val_loss: 0.6583\n",
      "Epoch 32/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6332 - loss: 0.6301 - val_accuracy: 0.5633 - val_loss: 0.6571\n",
      "Epoch 33/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6312 - loss: 0.6308 - val_accuracy: 0.5700 - val_loss: 0.6583\n",
      "Epoch 34/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6220 - loss: 0.6245 - val_accuracy: 0.5667 - val_loss: 0.6560\n",
      "Epoch 35/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6361 - loss: 0.6138 - val_accuracy: 0.5767 - val_loss: 0.6555\n",
      "Epoch 36/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6297 - loss: 0.6199 - val_accuracy: 0.5700 - val_loss: 0.6539\n",
      "Epoch 37/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6415 - loss: 0.6319 - val_accuracy: 0.5600 - val_loss: 0.6541\n",
      "Epoch 38/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6411 - loss: 0.6170 - val_accuracy: 0.5667 - val_loss: 0.6545\n",
      "Epoch 39/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6517 - loss: 0.6124 - val_accuracy: 0.5700 - val_loss: 0.6564\n",
      "Epoch 40/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6511 - loss: 0.6167 - val_accuracy: 0.5600 - val_loss: 0.6565\n",
      "Epoch 41/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6371 - loss: 0.6163 - val_accuracy: 0.5533 - val_loss: 0.6556\n",
      "Epoch 42/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6394 - loss: 0.6294 - val_accuracy: 0.5700 - val_loss: 0.6575\n",
      "Epoch 43/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6589 - loss: 0.6196 - val_accuracy: 0.5500 - val_loss: 0.6566\n",
      "Epoch 44/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6661 - loss: 0.6199 - val_accuracy: 0.5633 - val_loss: 0.6549\n",
      "Epoch 45/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6430 - loss: 0.6200 - val_accuracy: 0.5433 - val_loss: 0.6590\n",
      "Epoch 46/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6531 - loss: 0.6261 - val_accuracy: 0.5567 - val_loss: 0.6573\n",
      "Epoch 47/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6483 - loss: 0.6115 - val_accuracy: 0.5600 - val_loss: 0.6568\n",
      "Epoch 48/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6485 - loss: 0.6287 - val_accuracy: 0.5700 - val_loss: 0.6557\n",
      "Epoch 49/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6640 - loss: 0.6181 - val_accuracy: 0.5600 - val_loss: 0.6552\n",
      "Epoch 50/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6741 - loss: 0.6018 - val_accuracy: 0.5667 - val_loss: 0.6591\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n",
      "Rapport de classification :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54       144\n",
      "           1       0.58      0.60      0.59       156\n",
      "\n",
      "    accuracy                           0.57       300\n",
      "   macro avg       0.57      0.57      0.57       300\n",
      "weighted avg       0.57      0.57      0.57       300\n",
      "\n",
      "Neural Network Accuracy: 0.5666666666666667\n",
      "Neural Network Precision: 0.5802469135802469\n",
      "Neural Network Recall: 0.6025641025641025\n",
      "Neural Network F1 Score: 0.5911949685534591\n"
     ]
    }
   ],
   "source": [
    "# implementing the Neural Network Model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Intializing model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding layers\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))  \n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(32, activation='relu'))  \n",
    "model.add(Dropout(0.3))  \n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "# Model Compilation\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),  \n",
    "              loss='binary_crossentropy',  \n",
    "              metrics=['accuracy']) \n",
    "\n",
    "y_train_NN = np.array(y_train_smote)\n",
    "y_test_NN = np.array(y_test)\n",
    "\n",
    "# Model training\n",
    "history = model.fit(X_train_scaled, y_train_NN, \n",
    "                    validation_data=(X_test_scaled, y_test_NN), \n",
    "                    epochs=50,  \n",
    "                    batch_size=32,  \n",
    "                    verbose=1)\n",
    "\n",
    "y_pred_NN = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "print(\"Rapport de classification :\\n\", classification_report(y_test_NN, y_pred_NN))\n",
    "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_NN))\n",
    "print(\"Neural Network Precision:\", precision_score(y_test, y_pred_NN))\n",
    "print(\"Neural Network Recall:\", recall_score(y_test, y_pred_NN))\n",
    "print(\"Neural Network F1 Score:\", f1_score(y_test, y_pred_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6814 - loss: 0.5872 - val_accuracy: 0.5900 - val_loss: 0.6643\n",
      "Epoch 2/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6716 - loss: 0.5973 - val_accuracy: 0.5833 - val_loss: 0.6688\n",
      "Epoch 3/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.5925 - val_accuracy: 0.5867 - val_loss: 0.6719\n",
      "Epoch 4/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6919 - loss: 0.5949 - val_accuracy: 0.5733 - val_loss: 0.6702\n",
      "Epoch 5/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6465 - loss: 0.5975 - val_accuracy: 0.5867 - val_loss: 0.6666\n",
      "Epoch 6/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6751 - loss: 0.5874 - val_accuracy: 0.5800 - val_loss: 0.6659\n",
      "Epoch 7/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6834 - loss: 0.5878 - val_accuracy: 0.5867 - val_loss: 0.6625\n",
      "Epoch 8/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6891 - loss: 0.5775 - val_accuracy: 0.5967 - val_loss: 0.6634\n",
      "Epoch 9/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6722 - loss: 0.5908 - val_accuracy: 0.5800 - val_loss: 0.6645\n",
      "Epoch 10/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6751 - loss: 0.5932 - val_accuracy: 0.5800 - val_loss: 0.6653\n",
      "Epoch 11/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6947 - loss: 0.5544 - val_accuracy: 0.5867 - val_loss: 0.6610\n",
      "Epoch 12/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6878 - loss: 0.5877 - val_accuracy: 0.5767 - val_loss: 0.6663\n",
      "Epoch 13/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7111 - loss: 0.5602 - val_accuracy: 0.5933 - val_loss: 0.6633\n",
      "Epoch 14/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7004 - loss: 0.5814 - val_accuracy: 0.5767 - val_loss: 0.6700\n",
      "Epoch 15/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7095 - loss: 0.5702 - val_accuracy: 0.5867 - val_loss: 0.6679\n",
      "Epoch 16/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6628 - loss: 0.5994 - val_accuracy: 0.5767 - val_loss: 0.6687\n",
      "Epoch 17/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6624 - loss: 0.5977 - val_accuracy: 0.5867 - val_loss: 0.6647\n",
      "Epoch 18/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6883 - loss: 0.5866 - val_accuracy: 0.5900 - val_loss: 0.6670\n",
      "Epoch 19/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7066 - loss: 0.5694 - val_accuracy: 0.5933 - val_loss: 0.6686\n",
      "Epoch 20/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6658 - loss: 0.5778 - val_accuracy: 0.5933 - val_loss: 0.6729\n",
      "Epoch 21/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6630 - loss: 0.5876 - val_accuracy: 0.5900 - val_loss: 0.6694\n",
      "Epoch 22/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6568 - loss: 0.5830 - val_accuracy: 0.5900 - val_loss: 0.6764\n",
      "Epoch 23/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6818 - loss: 0.5700 - val_accuracy: 0.5967 - val_loss: 0.6702\n",
      "Epoch 24/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6841 - loss: 0.5850 - val_accuracy: 0.5933 - val_loss: 0.6689\n",
      "Epoch 25/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7090 - loss: 0.5579 - val_accuracy: 0.5833 - val_loss: 0.6771\n",
      "Epoch 26/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6867 - loss: 0.5757 - val_accuracy: 0.5867 - val_loss: 0.6742\n",
      "Epoch 27/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6848 - loss: 0.5969 - val_accuracy: 0.5833 - val_loss: 0.6749\n",
      "Epoch 28/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6894 - loss: 0.5669 - val_accuracy: 0.5900 - val_loss: 0.6748\n",
      "Epoch 29/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6949 - loss: 0.5737 - val_accuracy: 0.5867 - val_loss: 0.6720\n",
      "Epoch 30/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7008 - loss: 0.5619 - val_accuracy: 0.5733 - val_loss: 0.6736\n",
      "Epoch 31/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6641 - loss: 0.5966 - val_accuracy: 0.5900 - val_loss: 0.6732\n",
      "Epoch 32/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6961 - loss: 0.5739 - val_accuracy: 0.5933 - val_loss: 0.6795\n",
      "Epoch 33/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6944 - loss: 0.5668 - val_accuracy: 0.5933 - val_loss: 0.6697\n",
      "Epoch 34/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7256 - loss: 0.5579 - val_accuracy: 0.5867 - val_loss: 0.6706\n",
      "Epoch 35/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6916 - loss: 0.5812 - val_accuracy: 0.5867 - val_loss: 0.6829\n",
      "Epoch 36/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7055 - loss: 0.5634 - val_accuracy: 0.5967 - val_loss: 0.6755\n",
      "Epoch 37/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6931 - loss: 0.5734 - val_accuracy: 0.5933 - val_loss: 0.6755\n",
      "Epoch 38/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6897 - loss: 0.5786 - val_accuracy: 0.5967 - val_loss: 0.6754\n",
      "Epoch 39/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7030 - loss: 0.5631 - val_accuracy: 0.5867 - val_loss: 0.6795\n",
      "Epoch 40/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6762 - loss: 0.5760 - val_accuracy: 0.5967 - val_loss: 0.6740\n",
      "Epoch 41/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7077 - loss: 0.5586 - val_accuracy: 0.6033 - val_loss: 0.6725\n",
      "Epoch 42/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7256 - loss: 0.5443 - val_accuracy: 0.5933 - val_loss: 0.6698\n",
      "Epoch 43/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6972 - loss: 0.5782 - val_accuracy: 0.5800 - val_loss: 0.6716\n",
      "Epoch 44/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7189 - loss: 0.5516 - val_accuracy: 0.5833 - val_loss: 0.6779\n",
      "Epoch 45/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6989 - loss: 0.5617 - val_accuracy: 0.5833 - val_loss: 0.6782\n",
      "Epoch 46/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7218 - loss: 0.5476 - val_accuracy: 0.5833 - val_loss: 0.6736\n",
      "Epoch 47/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6972 - loss: 0.5712 - val_accuracy: 0.5767 - val_loss: 0.6794\n",
      "Epoch 48/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7033 - loss: 0.5670 - val_accuracy: 0.5900 - val_loss: 0.6806\n",
      "Epoch 49/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6827 - loss: 0.5735 - val_accuracy: 0.5867 - val_loss: 0.6775\n",
      "Epoch 50/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6895 - loss: 0.5716 - val_accuracy: 0.5967 - val_loss: 0.6757\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Rapport de classification :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.56      0.57       144\n",
      "           1       0.61      0.63      0.62       156\n",
      "\n",
      "    accuracy                           0.60       300\n",
      "   macro avg       0.60      0.60      0.60       300\n",
      "weighted avg       0.60      0.60      0.60       300\n",
      "\n",
      "Neural Network Accuracy: 0.5966666666666667\n",
      "Neural Network Precision: 0.6086956521739131\n",
      "Neural Network Recall: 0.6282051282051282\n",
      "Neural Network F1 Score: 0.6182965299684543\n"
     ]
    }
   ],
   "source": [
    "# optimizing the parameters by ponderation of the loss function\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_NN), y=y_train_NN)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "model.fit(X_train_scaled, y_train_NN, validation_data=(X_test_scaled, y_test_NN), \n",
    "          epochs=50, batch_size=32, verbose=1, class_weight=class_weights)\n",
    "\n",
    "y_pred_NN = (model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "print(\"Rapport de classification :\\n\", classification_report(y_test_NN, y_pred_NN))\n",
    "print(\"Neural Network Accuracy:\", accuracy_score(y_test, y_pred_NN))\n",
    "print(\"Neural Network Precision:\", precision_score(y_test, y_pred_NN))\n",
    "print(\"Neural Network Recall:\", recall_score(y_test, y_pred_NN))\n",
    "print(\"Neural Network F1 Score:\", f1_score(y_test, y_pred_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Perte Entraînement')\n",
    "plt.plot(history.history['val_loss'], label='Perte Validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.title('Courbe de perte')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Accuracy Entraînement')\n",
    "plt.plot(history.history['val_accuracy'], label='Accuracy Validation')\n",
    "plt.xlabel('Époques')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Courbe d\\'accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can easily see that the neural Network doesn't adapt to the dataset with a very small level of accuracy even when trying to optimize the hyperparameters. One reason is that algorithms like Neural Networks require bigger datasets to be able to generalize which is the reason why Logistic Regression performs well here. Also, Neural Network might be too complex for no reason because in my dataset there might not be too complicated relationships between my features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to understand better why Logistic Regression provided the best results after being optimized. It is a simple model and performs well with linear relationships. The features I created in the beginning were thus adapted and very useful. Using SMOTE allowed to rebalance the classes in a way that made the accuracy, recall and thus generalization increase. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
